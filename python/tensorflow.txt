# Install
https://www.tensorflow.org/versions/r0.12/get_started/os_setup
export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp27-none-linux_x86_64.whl
pip install upgrade $TF_BINARY_URL

# Pairwise comparison
a == 0  does not perform pairwise comparision but checks if a is 0
[0, 1, 0] == 0 // False
tf.equal([0, 1, 0], 0) // [True, False, True]
Same for tf.not_equal !=


# Misc
vector[:, None] // reshape vector to column vector; same as tf.reshape(vector, (-1, 1))
tf.reset_default_graph()  // clears everything
tf.py_func(fun, tensor) // wrap Python function
tf.truediv(x, y) // division
tf.minimum(x, y) // element-wise minimum; see also reduce_min()
tf.product(x, y) // element-wise product
tf.reduce_min(t, axis)
tf.reduce_max(t, axis)
tf.maximum(x, y) // element-wise maximum; see also reduce_min()
tf.train.exponential_decay(var, step, steps_per_decay, decay_rate, ...)
 * var = var * decay, every steps_per_decay
  staircase=True // decay as discrete intervals
tf.one_hot([1, 2, 0, 3], 4) // one-hot encode ints with max 4; return 2d array

# Creating tenors
tf.ones((3, 4))
tf.ones_like(t) // creates tensor of ones with same shape and dtype as t
tf.zeros((3, 4))
tf.zeros_like(t)
tf.fill((3, 4), value)
tf.constant(value, dtype=tf.float32, shape=(3, 4))
tf.range(min, max)
tf.linspace(min, max)


# Transformations
tf.reshape(x, [-1, 16])
tf.reshape(x, [-1]) // flatten

tf.concat([v1, v2], axis)
  * concat vectors without adding dimension
  * if v1/v2.shape = [n,], then axis=1 does not work (use stack)
tf.stack([v1, v2], axis=0)
  * Add dimension compared to concat
  * v1, v2 are vectors (shape=[n,]) -> result in [n, 2] for axis=1
tf.tile(x, [1, 2]) // copy columns twice
tf.expand_dims(x, dim) // insert single dimension/axis at dim in tensor
tf.squeeze(x, axis=[]) // squeeze all 1 dim axis, or select axis
  axis=[1, 3, 4] // only squeeze these axis
tf.transpose(x, perm=[1, 0, 2]) // exchange axis 0 and 1

# Reductions
tf.reduce_sum(t, axis)
tf.reduce_prod(t, axis)
tf.reduce_mean(t, axis)
tf.reduce_all(t, axis) // logical AND
tf.reduce_any(t, axis) // logical OR
tf.reduce_min(t, axis)
tf.reduce_max(t, axis)

## Conversion
tf.convert_to_tensor(array, dtype='float32') // np.array, list, scalar -> tensor
tensor.eval() // tensor -> array
tf.cast(t, dtype=tf.float32) // change dtype of tensor
tf.to_float(t) // cast to tf.float32


# Tensor / Variable
tf.Variable(10, name='var')
tf.get_variable('var', shape=(10), initializer=tf.constant_initializer(10))
with tf.variable_scope('head1'):
  sigma = tf.get_variable('sigma', shape=(10), initializer=tf.constant_initializer(10)) // sigma.name == 'head1/sigma'
with tf.variable_scope('head1', reuse=True):
  sigma = tf.get_variable('sigma') // retrieve/get existing variable; only work when created with get_variable, not Variable!
sigma = tf.get_variable('head1/sigma') // same as before


x.shape_as_list() // get shape
x.get_shape().as_list() // get shape
x.get_shape().ndim // ndim as scalar
tf.rank(x) // rank as tensor; similar to len(shape_as_list); ndim does not exist
x = tf.Variable(init, ...)
  init = value [1, 2, 3] or Tensor
  trainable=True // set False to exclude from training
sess.run(x) or x.eval(sess) // get value of variable
value = x.value() // get value of variable as op
assign_op = x.assign(value); sess.run(assign_op) // assign, set value variable
x.name // var:0
x.op.name // var

## Namespace variables
with tf.variable_scope('layer1'):
  // prepends 'layer1/' to variable names; same as using name='layer1/x'
  w = tf.get_variable('w', shape, dtype='float32',
                      initializer=tf.random_normal_initializer()
                      collections=[tf.GraphKeys, 'filters'])
  // w.name == 'layer1/a'

  b = tf.get_variable('b', shape,
                      initializer=tf.zeros_initializer, // not brackets!
                      collections=[tf.GraphKeys, 'biases']

## Scope
* Scope add adds just prefix to variable names (see below)

with tf.variable_scope('head', ...): // variables and ops
  a = tf.get_variable(0, name='a') // same as tf.Variable(0, name='head/a')
  trainable=True
  reuse=True // a = get_variable('name'); retrieve existing variable 'head/name'

with tf.name_scope(...) // ops
tf.get_variable_scope() // return current variable scope




# Collections
* container for variables identified by string
v = tf.Variable(0, 'v', collections=['c1'])
v = tf.get_variable('v', collections=['c1'])
  * adds variable to collection 'c1'
  * creates c1 if non-existing
tf.add_to_collection('c1', tensor) // add any tensor to collection
tf.get_collection_ref('c1')
  * returns list with tensors in collection
  * can be modified, e.g. via list.remove or list.append

## tf.GraphKeys() // default collection names
.VARIABLES

.GLOBAL_VARIABLES // tf.global_variables(); shared across machines
.MODEL_VARIABLES // tf.model_variables()
.TRAINABLE_VARIABLES // tf.trainable_variables(); contained in GLOBAL_VARIABLES
// Convention: GLOBAL_VARIABLES -> MODEL_VARIABLES -> TRAINABLE_VARIABLES
// However, variables not automatically added to GLOBAL_VARIABLES when collection=[tf.GraphKeys.MODEL_VARIABLES]!

.LOCAL_VARIABLES // tf.local_variables(); local on machine
.SUMMARIES // used, e.g. by tf.summary.merge_all
.GLOBAL_STEP



# Retrieve objects
## Variables
for _ in tf.all_variables():
  print(_.name)

## Operations and tensors
for _ in graph.get_operations(): // Operations and placeholders
  print(_.name)
  * 'name:0' will be first output
graph.get_operation_by_name('inception_v3/inception_v3/conv0/Relu') // operation, not output tensor!
op.graph // graph of op
graph.get_tensor_by_name('inception_v3/inception_v3/conv0/Relu:0') // output tensor
sess.run('inception_v3/inception_v3/conv0/Relu:0', feed_dict=) // Evaluate tensor by name






# Initializer
tf.constant_initializer(value)
tf.random_uniform_initializer(a, b) // [a, b]
tf.random_normal_initializer(mean, std)
tf.zeros_initializer // no ()
tf.contrib.layers.xavier_initializer


# Saving/Restoring variables
saver = tf.train.Saver(...)
  {'v1': v1, 'v2': v2} // select variables to store
  max_to_keep=5 // max # checkpoints if global_step is used
saver.save(session, path='./dir/prefix', ...)
  path: dirname + prefix; files written to '/dir/prefix00...'
  global_step=int: index appended to `path`
saver.restore(session, 'model.ckp')

## tf.contrib.framework
assign_from_checkpoint_fn(...)
  * Creates function fun(sess) with session argument
  model_path=tf.train.latest_checkpoint(checkpoint_dir),
  var_list=None // to select variables; e.g. tf.model_variables()
  ignore_missing_vars=False

init_from_checkpoint(path, assign_map)
  * Creates init op
  path // checkpoint path
  assign_map // {to: from} dict; required
  assign_map={v.name.split(':')[0]: v for v in tf.model_variables()}

list_variables(path)
  * list variables in checkpoint




## Example
graph = build_graph() // must be build before
sess = tf.Session()
saver = tf.train.Saver() // only stores/restored variables that are in graph
saver.restore(sess, './checkpoint/00') // restore previous model
saver.save(sess, ./checkpoints/01') // save new model


# conv2d
tf.nn.conv2d(x, filter, strides, padding)
  x = rf.reshape(x, [-1, height, width, nb_channel])
  x = tf.placeholder('float32', [None, height, width, nb_channel])
  filter = tf.Variable(tf.random.normal([height, width, nb_in-channel, nb_out-channel])
  strides = [1, height, width, 1] // 1 over samples and channels
  padding = 'SAME' (preserve dim by zero-padding), 'VALID' without zero-padding

# pooling
tf.nn.max_pool(x, k=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')

# dropout
rf.nn.dropout(x, keep_prob=0.7) // dropout with 0.3

# Interactive session
sess = tf.InteractiveSession()
operations = tf.global_variables_initializer()
operations.run() // instead of sess.run(operations)
variable.eval() // intead of sess.run(variable)

# control_dependencies
* evaluates op1 and op2 before opx is evaluated
with tf.control_dependencies([op1, op2]):
  opx = other_opp



# tf.ExponentialMovingAverage
ema = tf.ExponentialMovingAverage(decay)
average_op = ema.apply(vars)
  * Create new vars_ema that are the EMA of vars
  * vars_ema are added to tf.get_collection(tf.GraphKeys.MOVING_AVERAGE_VARIABLES)

average_var = ema.average(var)
  * Return averaged version of var
  * or None if ema.apply([var]) has not been applied / var not in EMA collection

tf.moving_average_variables()
  * Same as tf.get_collection(tf.GraphKeys.MOVING_AVERAGE_VARIABLES)

ema.variables_to_restore(var_list)
  * var_list = moving_average_variables() + trainable_variables()
  * Returns dict from restore names to variables
  * If variable has EMA variable, will map to that variable
  * Otherwise will map to non-EMA variable
  * --> Tries to restore EMA variables if existing

def update_averages():
  _ = ema.apply([mu, sigma]) // op that will eval mu and sigma, and add to avg
  with tf.control_dependencies([_]): // first execute _
    return (tf.identity(mu), tf.identify(sigma)) // return updated mu, sigma


# Tensorboard
tensorboard --logdir path // path containes [train,test_eval,train_eval] dirs
  --logdir m1:./model1/train,m2:model2/train
  --port 6006

writer = tf.summary.FileWriter(out_dir)
s1 = tf.summary.scalar(name, tensor)
s2 = tf.summary.histogram(name, tensor)
merged = tf.summary.merge_all()
merged = tf.summary.merge([s1, s2])
summaries = sess.run(merged, feed_dict=...)
writer.add_summary(summaries, global_step)







# Graphs
g0 = tf.get_default_graph() // returns default graph
g = tf.Graph()
with g.as_default():
  a = tf.Variable(...) // variable of g, not g0
assert a.graph is g
assert a.graph is not g0

with g.as_default():
  sess = tf.Session() // will use g
  sess.run(op_in_g)

tf.Session(graph=graph)
tf.InteractiveSession(graph=graph)


variable.get_graph()
session.get_graph()

# QueueRunners
coord = tf.train.Coordinator()
threads = tf.train.start_queue_runners(sess, coord)
coord.request_stop()
coord.join(threads)
sess.close()

# Devices
tf.device(name): // assign to certain device instead of choosing automatically
  /cpu:0 // 1st cpu
  /cpu:1 // 2nd cpu
  /gpu:0 // 1st gpu
  /job:worker/taskX // assign to task X
tf.Session(config=tf.ConfigProto(log_device_placement=True)) // log device placement by run

# IO
1. Feeding from file
sess.run(x, feed_dict={external_data})

2. Reading from file
train.match_filenames_once
train.string_input_producer

filename_queue = train.string_input_producer(['file1.np', 'file2.np'])

# Computing gradients
[grad_x] = tf.gradients(loss, x)
  * Gradient of loss wrt. x
  * loss/x can be list of single tensor
  * always returns list!

# Optimizer
opt = tf.train.AdamOptimizer()
opt.compute_gradients(loss, variables=None)
  * returns [(gradient, variable_name), ...]
  variables=tf.Variable(...)   // wrt. certain tf Variables
opt.apply_gradients([(grad1, var1), (grad2, var2)])

# Clipping gradients
* tf.clip_by_norm(gradient, clip_norm) // clips single tensor
* tf.clip_by_global_norm(t_list, clip_norm) // clips list of tensors by sum of gradients
  - t_list[i] = t_list[i] * clip_norm / max(clip_norm, global_norm)
  - global_norm = sqrt(sum([l2norm(t)**2 for t in t_list]))
  - if global_norm > clip_norm, downscale by ratio
  - better than clip_by_norm, but slower

for idx, gradient in enumerate(gradients):
  gradients[idx] = tf.clip_by_norm(gradient, max_norm)
gradients, global_norm  = tf.clip_by_global_norm(gradients, global_norm)


# Transformations
tf.slice(tensor, [x, y], [size_x, size_y]) // slice block at (x, y) of size
tf.gather(tensor, indices)
  indices=[1, 2] // tensor[1], tensor[2] // selects rows
tf.gather_nd(tensor, indices)
  indices=[[x1, y1], [x2, y2]] // selects elements at x1, y1


# Operations
tf.nn.xw_plus_b(x, w, b) // x * w + b

# Image operations
resize_images(image, width, height, ...)
  method=tf.image.ResizeMethod.BILINEAR
crop_center(image, 0.5) // image in 3d; shape of results [?, ?, 3]
pad_to_bounding_box(image, offset_x, offset_y, width, height)
  offset_[xy]: # rows that are filled with 0 left/above
  width, height: size of results image
sample_distorted_bounding_box // samples multiple bbox from single 3d image
extract_glimpse(input, size, offset, ...) // extracts glimpses; fills rest with noise
  input: 4d tensor [batch_size, width, height, channels]
  size: [glimpse_width, glimpse_height]
  offset: 2d tensor [batch_size, x_offset - y_offset]


# RNN
* Use static_rnn or dynamic_rnn, not raw BasicLSTMCell!
* tf.nn.dynamic_rnn: variable batch size (can be None) -> preferred!
* tf.contrib.static_rnn: fixed number of time steps
* Note: arguments of dynamic_rnn and static_rnn differ !!
* tf.unstack: transforms tensor axis into list

## dynamic_rnn
* batch_size can be None
* returns only final state!
outputs, state = tf.nn.dynamic_rnn(cell, inputs, ...)
  cell: RNNCell
  inputs: tensor [batch_size, time_steps, features]
  initial_state: tensor [batch_size, features]

## static_rnn
 outputs, states = rnn.static_rnn(cell, inputs, ...)
  cell: RNNCell
  inputs: list! -> tf.unstack(inputs), where first axis of inputs are time steps
  initial_state: tensor [batch_size, nb_unit] for dtype

tf.reset_default_graph()
inputs = tf.placeholder(shape=[nb_step, batch_size, nb_feat], dtype=tf.float32)
cell = tf.contrib.rnn.BasicLSTMCell(nb_unit)
outputs, states = tf.contrib.rnn.static_rnn(cell, tf.unstack(inputs), dtype=tf.float32)
q_values = slim.fully_connected(outputs, nb_action, activation_fn=None)



# arg_scope
arg_scope = tf.contrib.framework.arg_scope
* Most inner arg_scopre prevails:
  with arg_scope([layers.dropout], is_training=False):
    with arg_scope([layers.dropout], is_training=True):
      net = layers.dropout(net) // is_training=True


# printing / logging
var = tf.Print(var, [var], 'Values of var: ')




# Estimator

## Estimator
* Build graph and session once train() / eval() is called
estimator = Estimator(
  model_fn= // model_fn that returns EstimatorSpec
  params= // dict with params passed to train() / evaluate()
  config= // RunConfig
  model_dir=None // path to model dir
)

### Attributes
.config // RunConfig
.params // params dict
.model_dir
.latest_checkpoint() // function that returns path to latest checkpoint


## EstimatorSpec
def model_fn(features, labels, mode, params):
  return EstimatorSpec(
      mode=mode
      loss=loss,
      train_op=optimizer.minimize()
    )

## tf.estimator.inputs.numpy_input_fn(
tf.estimator.inputs.numpy_input_fn(

  x={'images': np.ones((batch, height, width, channels))}, // features
  y=None, // labels
  num_epochs=1,
  shuffle=False)
Returns input_fn where input_fn() returns (features(=x), labels(=y))






# Dataset

## Basics
dset = tf.data.Dataset.from_tensor_slices([tensor1, tensor2])
  * single tensor, list of tensors, or (list of) dict
iter = dset.make_one_shot_iterator()
data = iter.get_next()


## Creating dataset
tf.data.Dataset.from_tensor_slices // from tensor or numpy array
tf.data.TextLineDataset(filenames) // each example is single line
tf.data.experimental.CsvDataset(filenames, ...) // to parse columns of CSV files
  * Same as TextLineDataset + tf.decode_csv
zip((dset1, dset2)) // zips outputs of multiple datasets like Python zip
list_files('path/*glob*') // returns dset of string filenames


## Iterators

### One-shot
* Not initialized
iter = make_one_shot_iterator()

## Initializable iterator
* Initialized
* if input contains tf.Variables or tf.placeholder
iter = make_initializable_iterator()
sess.run(iter.initializer) #op to initializer

## Reinitializable iterator
* Two merge multiple dataset with the same structure
iter = tf.data.Iterator.from_structure(
  dset.output_types, dset.output_shapes)
init1 = iter.make_initializable(dset)
init2 = iter.make_initializable(dset2)
sess.run([init1, init2])


## Functions
map(fun, num_parallel_calls=None) // applies fun to each element (sliced along axis=0)
apply(fun) // applies fun to dataset; fun takes dataset and returns dataset;
shuffle(buffer_size, seed=None, reshuffle_each_iteration=True)
  * shuffles buffer_size consecutive elements
  * bad shuffling if buffer_size small
  * if batch() applied before, shuffles within each batch!
repeat
prefetch(buffer_size) // prefetch elements into memory
  buffer_size=tf.contrib.data.AUTOTUNE
take(num_elements) // takes num_elements from dataset
batch(batch_size)
padded_batch(batch_size, padded_shapes=[None])
  * Pads all sequences in the same batch to the same (maximum) length
  * Sequences in different batches can have different shapes


## Other functions
tf.data.Dataset.list_files('/path/input_files*').apply(
  tf.data.experimental.parallel_interleave(
    lambda dset: dset.map(_preprocess_fn),
    cylce_lenth=num_threads,  # Number of parallel operations
    sloppy=False,   # If True, faster but non-deterministic
    )
  )
  * transforms dataset in parallel
  * slices dataset into cylce_length and applies in parallel to each dset
  * e.g. to process multiple input files in parallel


## Order of functions
map
shuffle
repeat
prefetch



# Eager executing
tf.enable_eager_execution   // add beginning of code
vn = np.random.rand(4, 2) // np.ndarray
vt = tf.constant(vn) // tf.EagerTensor

vt.numpy() // np.ndarray
np.asarray(vt) // np.ndarray
vn * vt // tf.EagerTensor
tf.add(vn, 1)  // tf.EagerTensor
np.add(vt, 1) // np.ndarray

## Computing gradients
with tf.GradientTape() as tape:
  loss = loss_fun(labels, predicts)
gradient = tape.gradient



# Keras
## losses
tf.losses.softmax_crossentropy(labels, preds, ...)
  labels  // one-hot encoded
  preds  // logits
tf.losses.sparse_softmax_crossentropy(labels, preds, ...)
  labels  // indices; not one-hot encoded!; one dimensions less than preds
  preds  // logits
keras.losses.categorial_crossentropy(labels, preds, ...)
  labels  // one-hot encoded
  preds  // probs; not logits!
  from_logits=False // if set to True, same as tf.losses.sparse_softmax_crossentropy
keras.backend.sparse_categorical_crossentropy(labels, preds, ...)
  * same as keras.losses.sparse_categorical_crossentropy
