# Install
https://www.tensorflow.org/versions/r0.12/get_started/os_setup
export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp27-none-linux_x86_64.whl
pip install upgrade $TF_BINARY_URL

# Pairwise comparison
a == 0  does not perform pairwise comparision but checks if a is 0
[0, 1, 0] == 0 // False
tf.equal([0, 1, 0], 0) // [True, False, True]
Same for tf.not_equal !=


# Misc
tf.reset_default_graph()  // clears everything
tf.py_func(fun, tensor) // wrap Python function
tf.truediv(x, y) // division
tf.minimum(x, y) // element-wise minimum; see also reduce_min()
tf.product(x, y) // element-wise product
tf.reduce_min(t, axis)
tf.reduce_max(t, axis)
tf.maximum(x, y) // element-wise maximum; see also reduce_min()
tf.train.exponential_decay(var, step, steps_per_decay, decay_rate, ...)
 * var = var * decay, every steps_per_decay
  staircase=True // decay as discrete intervals
tf.one_hot([1, 2, 0, 3], 4) // one-hot encode ints with max 4; return 2d array

# Creating tenors
tf.ones((3, 4))
tf.ones_like(t) // creates tensor of ones with same shape and dtype as t
tf.zeros((3, 4))
tf.zeros_like(t)
tf.fill((3, 4), value)
tf.constant(value, dtype=tf.float32, shape=(3, 4))
tf.range(min, max)
tf.linspace(min, max)


# Transformations
tf.reshape(x, [-1, 16])
tf.reshape(x, [-1]) // flatten

tf.concat([v1, v2], axis)
  * concat vectors without adding dimension
  * if v1/v2.shape = [n,], then axis=1 does not work (use stack)
tf.stack([v1, v2], axis=0)
  * Add dimension compared to concat
  * v1, v2 are vectors (shape=[n,]) -> result in [n, 2] for axis=1
tf.tile(x, [1, 2]) // copy columns twice
tf.expand_dims(x, dim) // insert single dimension/axis at dim in tensor
tf.squeeze(x, axis=[]) // squeeze all 1 dim axis, or select axis
  axis=[1, 3, 4] // only squeeze these axis

# Reductions
tf.reduce_sum(t, axis)
tf.reduce_prod(t, axis)
tf.reduce_mean(t, axis)
tf.reduce_all(t, axis) // logical AND
tf.reduce_any(t, axis) // logical OR
tf.reduce_min(t, axis)
tf.reduce_max(t, axis)

## Conversion
tf.convert_to_tensor(array, dtype='float32') // np.array, list, scalar -> tensor
tensor.eval() // tensor -> array
tf.cast(t, dtype=tf.float32) // change dtype of tensor


# Tensor / Variable
tf.Variable(10, name='var')
tf.get_variable('var', shape=(10), initializer=tf.constant_initializer(10))
with tf.variable_scope('head1'):
  sigma = tf.get_variable('sigma', shape=(10), initializer=tf.constant_initializer(10)) // sigma.name == 'head1/sigma'
with tf.variable_scope('head1', reuse=True):
  sigma = tf.get_variable('sigma') // retrieve/get existing variable; only work when created with get_variable, not Variable!
sigma = tf.get_variable('head1/sigma') // same as before


x.get_shape().as_list()
x = tf.Variable(init, ...)
  init = value [1, 2, 3] or Tensor
  trainable=True // set False to exclude from training
sess.run(x) or x.eval(sess) // get value of variable
value = x.value() // get value of variable as op
assign_op = x.assign(value); sess.run(assign_op) // assign, set value variable
x.name // var:0
x.op.name // var

## Namespace variables
with tf.variable_scope('layer1'):
  // prepends 'layer1/' to variable names; same as using name='layer1/x'
  w = tf.get_variable('w', shape, dtype='float32',
                      initializer=tf.random_normal_initializer()
                      collections=[tf.GraphKeys, 'filters'])
  // w.name == 'layer1/a'

  b = tf.get_variable('b', shape,
                      initializer=tf.zeros_initializer, // not brackets!
                      collections=[tf.GraphKeys, 'biases']

## Scope
* Scope add adds just prefix to variable names (see below)

with tf.variable_scope('head', ...): // variables and ops
  a = tf.get_variable(0, name='a') // same as tf.Variable(0, name='head/a')
  trainable=True
  reuse=True // a = get_variable('name'); retrieve existing variable 'head/name'

with tf.name_scope(...) // ops
tf.get_variable_scope() // return current variable scope




# Collections
* container for variables identified by string
v = tf.Variable(0, 'v', collections=['c1'])
v = tf.get_variable('v', collections=['c1'])
  * adds variable to collection 'c1'
  * creates c1 if non-existing
tf.add_to_collection('c1', tensor) // add any tensor to collection
tf.get_collection_ref('c1')
  * returns list with tensors in collection
  * can be modified, e.g. via list.remove or list.append

## tf.GraphKeys() // default collection names
.VARIABLES

.GLOBAL_VARIABLES // tf.global_variables(); shared across machines
.MODEL_VARIABLES // tf.model_variables()
.TRAINABLE_VARIABLES // tf.trainable_variables(); contained in GLOBAL_VARIABLES
// Convention: GLOBAL_VARIABLES -> MODEL_VARIABLES -> TRAINABLE_VARIABLES
// However, variables not automatically added to GLOBAL_VARIABLES when collection=[tf.GraphKeys.MODEL_VARIABLES]!

.LOCAL_VARIABLES // tf.local_variables(); local on machine
.SUMMARIES // used, e.g. by tf.summary.merge_all
.GLOBAL_STEP



# Retrieve objects
## Variables
for _ in tf.all_variables():
  print(_.name)

## Operations and tensors
for _ in graph.get_operations(): // Operations and placeholders
  print(_.name)
  * 'name:0' will be first output
graph.get_operation_by_name('inception_v3/inception_v3/conv0/Relu') // operation, not output tensor!
graph.get_tensor_by_name('inception_v3/inception_v3/conv0/Relu:0') // output tensor
sess.run('inception_v3/inception_v3/conv0/Relu:0', feed_dict=) // Evaluate tensor by name






# Initializer
tf.constant_initializer(value)
tf.random_uniform_initializer(a, b) // [a, b]
tf.random_normal_initializer(mean, std)
tf.zeros_initializer // no ()
tf.contrib.layers.xavier_initializer


# Saving variables
saver = tf.train.Saver(...)
  {'v1': v1, 'v2': v2} // select variables to store
  max_to_keep=5 // max # checkpoints if global_step is used
saver.save(session, path='./dir/prefix', ...)
  path: dirname + prefix; files written to '/dir/prefix00...'
  global_step=int: index appended to `path`
saver.restore(session, 'model.ckp')

## Example
graph = build_graph() // must be build before
sess = tf.Session()
saver = tf.train.Saver() // only stores/restored variables that are in graph
saver.restore(sess, './checkpoint/00') // restore previous model
saver.save(sess, ./checkpoints/01') // save new model


# conv2d
tf.nn.conv2d(x, filter, strides, padding)
  x = rf.reshape(x, [-1, height, width, nb_channel])
  x = tf.placeholder('float32', [None, height, width, nb_channel])
  filter = tf.Variable(tf.random.normal([height, width, nb_in-channel, nb_out-channel])
  strides = [1, height, width, 1] // 1 over samples and channels
  padding = 'SAME' (preserve dim by zero-padding), 'VALID' without zero-padding

# pooling
tf.nn.max_pool(x, k=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')

# dropout
rf.nn.dropout(x, keep_prob=0.7) // dropout with 0.3

# Interactive session
sess = tf.InteractiveSession()
operations = tf.global_variables_initializer()
operations.run() // instead of sess.run(operations)
variable.eval() // intead of sess.run(variable)

# control_dependencies
* evaluates op1 and op2 before opx is evaluated
with tf.control_dependencies([op1, op2]):
  opx = other_opp

# tf.ExponentialMovingAverage
ema = tf.ExponentialMovingAverage(decay)

def update_averages():
  _ = ema.apply([mu, sigma]) // op that will eval mu and sigma, and add to avg
  with tf.control_dependencies([_]): // first execute _
    return (tf.identity(mu), tf.identify(sigma)) // return updated mu, sigma


# Tensorboard
tensorboard --logdir path // path containes [train,test_eval,train_eval] dirs
  --logdir m1:./model1/train,m2:model2/train
  --port 6006

writer = tf.summary.FileWriter(out_dir)
s1 = tf.summary.scalar(name, tensor)
s2 = tf.summary.histogram(name, tensor)
merged = tf.summary.merge_all()
merged = tf.summary.merge([s1, s2])
summaries = sess.run(merged, feed_dict=...)
writer.add_summary(summaries, global_step)







# Graphs
g0 = tf.get_default_graph() // returns default graph
g = tf.Graph()
with g.as_default():
  a = tf.Variable(...) // variable of g, not g0
assert a.graph is g
assert a.graph is not g0

with g.as_default():
  sess = tf.Session() // will use g
  sess.run(op_in_g)

tf.Session(graph=graph)
tf.InteractiveSession(graph=graph)


variable.get_graph()
session.get_graph()

# QueueRunners
coord = tf.train.Coordinator()
threads = tf.train.start_queue_runners(sess, coord)
coord.request_stop()
coord.join(threads)
sess.close()

# Devices
tf.device(name): // assign to certain device instead of choosing automatically
  /cpu:0 // 1st cpu
  /cpu:1 // 2nd cpu
  /gpu:0 // 1st gpu
  /job:worker/taskX // assign to task X
tf.Session(config=tf.ConfigProto(log_device_placement=True)) // log device placement by run

# IO
1. Feeding from file
sess.run(x, feed_dict={external_data})

2. Reading from file
train.match_filenames_once
train.string_input_producer

filename_queue = train.string_input_producer(['file1.np', 'file2.np'])

# Optimizer
opt = tf.train.AdamOptimizer()
opt.compute_gradients(loss, variables=None)
  * returns [(gradient, variable_name), ...]
  variables=tf.Variable(...)   // wrt. certain tf Variables
opt.apply_gradients([(grad1, var1), (grad2, var2)])

# Clipping gradients
* tf.clip_by_norm(gradient, clip_norm) // clips single tensor
* tf.clip_by_global_norm(t_list, clip_norm) // clips list of tensors by sum of gradients
  - t_list[i] = t_list[i] * clip_norm / max(clip_norm, global_norm)
  - global_norm = sqrt(sum([l2norm(t)**2 for t in t_list]))
  - if global_norm > clip_norm, downscale by ratio
  - better than clip_by_norm, but slower

for idx, gradient in enumerate(gradients):
  gradients[idx] = tf.clip_by_norm(gradient, max_norm)
gradients, global_norm  = tf.clip_by_global_norm(gradients, global_norm)


# Transformations
tf.slice(tensor, [x, y], [size_x, size_y]) // slice block at (x, y) of size
tf.gather(tensor, indices)
  indices=[1, 2] // tensor[1], tensor[2] // selects rows
tf.gather_nd(tensor, indices)
  indices=[[x1, y1], [x2, y2]] // selects elements at x1, y1


# Operations
tf.nn.xw_plus_b(x, w, b) // x * w + b

# Image operations
resize_images(image, width, height, ...)
  method=tf.image.ResizeMethod.BILINEAR
crop_center(image, 0.5) // image in 3d; shape of results [?, ?, 3]
pad_to_bounding_box(image, offset_x, offset_y, width, height)
  offset_[xy]: # rows that are filled with 0 left/above
  width, height: size of results image
sample_distorted_bounding_box // samples multiple bbox from single 3d image
extract_glimpse(input, size, offset, ...) // extracts glimpses; fills rest with noise
  input: 4d tensor [batch_size, width, height, channels]
  size: [glimpse_width, glimpse_height]
  offset: 2d tensor [batch_size, x_offset - y_offset]


# RNN
* Use static_rnn or dynamic_rnn, not raw BasicLSTMCell!
* tf.nn.dynamic_rnn: variable batch size (can be None) -> preferred!
* tf.contrib.static_rnn: fixed number of time steps
* Note: arguments of dynamic_rnn and static_rnn differ !!
* tf.unstack: transforms tensor axis into list

## dynamic_rnn
* batch_size can be None
* returns only final state!
outputs, state = tf.nn.dynamic_rnn(cell, inputs, ...)
  cell: RNNCell
  inputs: tensor [batch_size, time_steps, features]
  initial_state: tensor [batch_size, features]

## static_rnn
 outputs, states = rnn.static_rnn(cell, inputs, ...)
  cell: RNNCell
  inputs: list! -> tf.unstack(inputs), where first axis of inputs are time steps
  initial_state: tensor [batch_size, nb_unit] for dtype

tf.reset_default_graph()
inputs = tf.placeholder(shape=[nb_step, batch_size, nb_feat], dtype=tf.float32)
cell = tf.contrib.rnn.BasicLSTMCell(nb_unit)
outputs, states = tf.contrib.rnn.static_rnn(cell, tf.unstack(inputs), dtype=tf.float32)
q_values = slim.fully_connected(outputs, nb_action, activation_fn=None)
