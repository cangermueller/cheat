# Infos / Memory usage / efficiency
df.info()
df.info(memory_usage='deep')
df.memory_usage()
pd.to_numeric(values, ...) // tries to make values numeric
  downcast='integer|signed|unsigned|float' // downcast to the smallest possible unit



Creating Data.Frame
===================
DataFrame({'id': [1, 2, 3], 'value': 10}) // from dict
DataFrame(odict, columns=odict.keys()) // from ordered dict preserving order
DataFrame([[11, 12],
           [21, 22]]) // from (numpy) array
pd.DataFrame.from_records([{'a': 1, 'b': 2}, {'a': 1, 'b': 2}])
 == pd.DataFrame({'a': [1, 1], 'b': [2, 2]})
pd.DataFrame.from_items([('a', [1, 2]), ('b', [2, 3])])  // from (key, [values]) pairs; values must be arrays



String functions
================
* object is the default dtype for string
* Use dtype='string' or dtype=pd.StringDtype to explicitly mark Series as strings
* Use frame.convert_dtypes() or convert all `object` types to `pd.StringDtype`


* Use pd.stringdtype
s = pd.Series(['1', 'ab', '10.0'])
>> s.str.fun. <<
isdigit == isnumeric == isdecimal // True if *positive int*
  * False for -123, 123.3
  * Same as '123'.isnumeric(), ...

isalpha // a-z
isalnum // a-z0-9
replace(what, by, ...)
  * what can be regex, e.g. '^chr'
  * matches anywhere
  * col.str.replace(r'.*_m-([^_]+).*', '\\1')
  case=False // case insensitive
slice_replace(from, to, replace)
slice(from, to)
pad(length, fillchar=' ') // extend length
startswith(string)  // starts with string (not re!)
fullmatch(re, ...) // matches full string
match(re, case=True, flags=0) // == re.match; matches at beginning
contains(re)  // == re.search; matches anywhere
  na=None // to specify the result of NaN value
extract('([a-b]+).*(\d+)', expand=True) // extract pattern and return df
extract('(?P<name1>[a-b]+).*(?P<name2>\d+)', expand=True) // extract pattern and return df with columns 'name1' and 'name2'

find(str, start=0, end=len) // -1 if not find; start else
lower()
upper()
split('_', expand=True) // split by pattern and return new frame
  t = d.col.str.split('_', expand=True)
  t.columns = ['a', 'b']
  d = pd.concat([d, t])
decode('utf') // bytes -> unicode
cat(sep=":', na_rep='') // concatenate values
  sep=  // field separator
  na_rep=None // if None, to sep is inserted / values ignored


# Delete, exclude, drop columns
d.drop(['c1', 'c2'], axis=1, inplace=True)
d.drop(columns=['c1', 'c2'], ...)
  errors='ignore' // don't raise exception if columns don't exist


# assign
d.assign(
  a=10,
  b=20,
  c=lambda x: x.a / x.b // function of other variables
  )

# Operations
df.corr(...)  // computes correlation between all columns
  -> use scipy correlation features for correlating specific columns
df.apply(fun, axis=0) // apply function over axis
  axis=0  // over rows -> return value for each column
  axis=1 // over columns -> return value for each row

# sample DataFrame
df.sample(n, ...)
  n=100 // number of samples
  frac=0.1 // fraction of samples instead of n
  replace=False // sample with/without replacemane
  axis=0
df.iloc[np.random.sample(df.index, n), :]
  np.random.sample(list, n) // sample n elements without replacement

# dtypes
df.dtypes // list dtypes
df = df.astype(np.int32)  // convert dtype of DataFrame
df['c1'] = df['c1'].astype(str) // convert dtype column
df.select_dtypes(np.int32) // select all int columns

# Misc
pd.crosstab(y, x) // contingency table
df.filter(regex='e$') // select columns by regex.
df.filter(['col1', 'col2'])
df.filter(...)
  items=['c1', 'c2'] // select by list of items (exact match)
  regex='e$' // select by regex using re.search -> partial match
  axis=None // =1 by default for DataFrame


# IO
to_pickle('file.pkl')
read_pickle('file.pkl')
read_excel('file', 'sheet')
read_hdf('file', 'groupname')
read_table(file, ...) / read_csv(file, ...)
  read_table(sys.stdin, ...)  // read from stdin
  read_table(StringIO(string))  // Read from string
    from io import StringIO
  nrows // max number of rows to be read
  sep=',' // field separator
  sep='\s+' // separate by spaces
  usecols=['a', 'b'] // select columns by name
  usecols=[0, 1, 2] // select columns by index
  header=None // no header; not header=False!
  header=i  // row number to use as column names (default: 0)
  comment='#' // skip lines beginning with #; ignore everything after #
  skiprows=0  // skip that many rows at beginning
  dtype={'c1': 'str', ...} // dtype of columns by name
  dtype={0: 'str', ...} // dtype of columns by index
  dtype='str' // convert all columns to objects (strings)
  compression='gzip' // compress
  chunksize=100 // read in chunks with 100 rows (see below)
to_csv(file, ...)
  sep=','
  index=True  // write index
  header=False  // do not write header
  mode='a'  // append
  float_format='%.2f'
  compression="gzip"/None // compress files
to_string // aligns columns; only use if to_csv() does not work
  justify='left' // align columns left
to_sql('table1')  // see SQL below


# Reading large file / chunking
reader = pd.read_X(..., chunksize=10)
for chunk in reader:
  assert len(chunk) <= 10
chunk = reader.get_chunk(3)


# HDF
df.to_hdf('/df', format='t', ...)
  format='f'  // (default), fixed format; fast read + write; no query
  format='t'  // table format; slower write (+ read); query
  complib='bsoc', complevel=5 // compress
  complib='bsoc, bzip2, ...'
  complevel=[0-9] // 0 no compression (default)
pd.read_hdf('file.h5', 'name', ...)
  columns=['c1', 'c2']
  ## selection
  !!! format='t' required, (data_columns=True optional)
  where=["columns=['c1', 'c2'] & index>10]
  start=first_row // number (not index) first row (included)
  stop=last_row // number (not index) last row (excluded)
  for dc in pd.read_hdf(..., chunksize=100): // read in blocks
    dc.shape[0] == 100

## HDFStore
store = HDFStore(filename, ...)
  mode='a'  // r, w
store.close()
store.is_open
store.keys()
store.groups()

### Reading
store['df'], store.df // read DataFrame
store.select('df', '(index=[0, 1]) & ~(column=[A, B])')
  index > 10
store.remove('df', ...) // remove table
  where='index="A"'
  where='c1 > 10'

### Writing
store['df'] = df  // write DataFrame in fixed format
store.put('df', df, ...)
  format='f'  // fixed format: fast write, no query/append/delete
  format='t'  // table format: slower write, read still fast
    * required for selection
  data_columnns=True  // for queries where('c1 > 0.2')
store.append('df', df)  // append rows; create new if not existing

### Splitting into multiple table -> higher performance
store.append_to_multiple({'t1', ['c1', 'c2'], 't2': None, t, selector='t1')
  * arg1: how columns are split; None means all other columns
    * split by index not supported
  * t is DataFrame that is split
store['t1'] // read chunk
store.select_as_multiple(['t1', 't2'])  // read all chunks

### Selecting groups
* better: h5py
g = store['/group']
g._v_name // name
g._v_children // all children
g._v_groups // all sub-groups
g._v_parent // parent



### Queries
* format='t' required!
* data_columns=True required for querying by value
* read_hdf(where=query)
* store.select('df', query)
* http://pandas.pydata.org/pandas-docs/dev/io.html#querying-a-table
index < 5 and columns=='c1' // select by index, columns
index in [1, 2] & ~(columns in ['c1', 'c2']) // select by index, columns
c1 < 10 // select by value; requires data_columns=True
'c1 < = %d' % (variable)  // not direct reference possible (see query())







### rhdf5
* supports different data types
* columns are arrays! : num [1:222092(1d)]
  -> unlist(d[,c])
store.put('df', df, format='t', data_columns=True)
df <- h5read('file.h5', '/df/table')
df <- h5read('file.h5', 'df')$table

#### Python
def join_index(index, sep='_'):
    return [sep.join(x) for x in index.values]

def to_r_hdf(d, filename, group):
    if d.columns.nlevels > 1:
        d = d.copy()
        d.columns = join_index(d.columns)
    d = d.reset_index()
    d.to_hdf(filename, group, format='t', data_columns=True)

#### R
h5_read <- function(filename, group, ...) {
  d <- suppressWarnings(h5read(filename, group, ...)$table)
  for (i in 1:ncol(d)) {
    d[,i] <- as.vector(d[,i])
  }
  d <- d %>% tbl_df %>% select(-index)
  return (d)
}




# feather
* Good for exchanging data between R and Python
* See feather.txt
* alternative: parquet
d.to_feather('data.feather')
d.to_parquet('data.parquet')








# Creating index
i = Index(array, name=, dtype=
df = DataFrame(array, index=i, columns=i)
df.index = i / df.columns = i
i.name; i.names // return names
i = i.set_names('name', ...) // change name; returns new index
  inplace=False


# Multiindex / Hierarchical index
u = MultiIndex.from_arrays([array1, array2], names=[name1, name2])  // must have same length
i = MultiIndex.from_tuples([('chr1', 1), ('chr1', 2), ]) // must have same length
i = MultiIndex.from_arrays(...) // same as from_tuples
i = MultiIndex.from_product([[o1, o2], ['i1', 'i2', 'i3']], names=['chromo', 'pos'])
  * Make index of Cartesian product of two iterators
i.levels[k] // unique index values at level k
  * only Multiindex
i.get_level_values(k) // index values (not unique) at level k
i.droplevel(k) // remove level k

# Index options
.nlevels

## Merge  / join hierarchical columns
df.columns = df.columns.map('_'.join)


# Index operations
rename(...)
  columns={'old': 'new'}  // rename columns
  index={'old':'new'} // rename rows
  columns=lambda x: x + '_' // apply renaming function
  inplace=True
set_axis(columns, axis=1) // set new columns (rename all columns)
rename_axis('key', axis=0) // rename index or columns; changes index.name
set_index([columns], ...) // set row index
  drop=False  // retain column
  append=True // append to end
  axis=1  // replace columns
reset_index(...)  // row index -> columns
  level= // reset only these levels; useful after groupby
  drop=True // do not add columns; only delete level
d = d.sort_index(...) // sort (hierarchical) index / columns
  axis=0 // rows or columns
  level=None // the level to be sorted; None means all levels
  ascending=True
swaplevel(0, 1, ...)  // swaps *two* levels
  inplace=False
reorder_levels([1, 0], ...) // change order of *all* levels
  axis=0
reindex(range(1990, 2000))  // adjust to new index
  * drop rows that are not indexed by new index
  * fill with np.NaN for newly indexed rows
  * columns=  // reindex columns
  * index=  // reindex index
  * fill_value=
  * method='nearest'  // use nearest value
droplevel(0) // drops level of multi-index; not in-place

# Check if index contains value
value in df.index
'a' in df.index // single-index
'a' in df.index // multi-index first level
('a', 1) in df.index  // mutli-index both levels

# Flatten index
def flatten_columns(d):
    d.columns = ['_'.join(x) for x in d.columns.values]



# Missing values
Use pd.NA: works with numeric and non-numeric values
pd.isna(pd.NA) == pd.isna(None) == pd.isna(np.nan)
np.nan only works for numeric values!
pd.isnull is pd.isna  // same functions; alias

notnull() == not isnull()
count() // # non-missing
fillna(0, ...) // fills na / null
  inplace=False
df.fillna(df.mean())  // fill / impute with column means; inplace!
df.fillna({'c2':10, 'c3':5})  // fill named columns
dropna(...) // drop rows with at least one missing value
  axis=0  // drop incomplete rows (default)
  how='any' // if any entry is missing <-> how='all'

## Misc
* Convention: Use np.nan for missing data, not None (also non-numeric data)
* np.nan is faster than None (dtype=float instead of dtype=object)
type(np.nan) -> float
type(None) -> NoneType
np.nan is np.nan
np.nan is not None
np.isnan(np.nan) -> True
np.isnan(None) -> Error
pd.isnull(np.nan) -> True
np.isnull(None) -> True


## Replacing / masking missing values
data.mask(data == 'nan', inplace=True)
  * data can be Series or DataFrame
  * works with integers and strings
!! Don't use data[data == 'nan'] = np.nan
  * np.nan converted to string 'nan'


## Interpolation
[1, np.nan, 3].interpolate() // -> [1, 1.5, 3]
  method='index'  // observations not equally spaced
count(axis=)  // count # non-null elements

# Replacing values
replace(from, to)
replace(np.nan, 1)
replace([f1, f2], [t1, t2]) // f1 -> t1; f2 -> t2
replace({f:t})
replace('\w+', '', regex=True)


# pandas printoptions
?set_option // lists all options
-1: infinite
set_option('display.width', 150)
  display.width=80 // line width
  display.max_colwidth=60 // max column width; longer will be truncated by ...
  display.max_rows=60
  display.max_columns=60
  display.precision=6
  display.float_format, lambda x: '%.3f' % x
get_option('display.width')

with option_context('display.max_rows', 10, 'display.max_columns', 5):
  ...
with pd.option_context('display.float_format', '{:,.2f}'.format):
  display(fit_scores)

## defaults
pd.set_option('display.width', 200)
pd.set_option('display.max_colwidth', -1)
pd.set_option('display.max_rows', 1000)


## shot full dataframe
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', -1)

# Reshaping
## Stacked -> unstacked
id | id2 | value
  0     0       3
  0     1       4
  1     0       2
  1     1       5
        =>
      id2
    id1 0  1
      0 3  4
      0 2  5
df.pivot_table(index='id1', columns='id2', values='value')
  * use pivot_table instead of pivot
df.pivot(index='id1', columns='id2')['value']
  * does not support aggregating values unlike pivot_table
  * index, columns is single string, not list!
  * Hierarchical column index with value column at level 0

## Unstacked -> stacked
pd.melt(df, ...) or df.melt(...)
pd.melt(df, id_vars='id1', var_name='id2', value_name='value')
pd.melt(df, ...) // matrix -> list
  id_vars= // columns that stay as they are
  var_name= // name of new key columns
  value_name= // name of new value column
pd.melt(['a', 'b', 'c'], id_vars='a', var_name='id', value_name='value')
  ['a', 'b', 'c'] -> ['a', 'id', 'value']


## Example
d = pd.concat([d1, d2])
  chromo  pos   value   sample
  0   1   3003339   1   CSC2_3B
  1   1   3003379   1   CSC2_3B
dmatrix = pd.pivot_table(index=['chromo', 'pos'], columns='sample', values='value')
       sample  CSC2_3B   CSC2_3C
  chromo  pos
  1   3003339        1   NaN
      3003379        1   NaN
      3003582        1   NaN
dlist = pd.melt(dmatrix.reset_index(), id_vars=['chromo', 'pos'], var_name='sample', value_name='value')
  chromo  pos   sample  value
  0   1   3003339   CSC2_3B   1
  1   1   3003379   CSC2_3B   1
  2   1   3003582   CSC2_3B   1


# Stacking, unstacking
df.stack() // columns become Mutliindex ~ melt
df.unstack(..) // last level becomes column ~ pivot_table
  * returns hierachrical index; use df.columns = df.colummns.get_level_values(0) to select one level
  level=-1  // level of index that will become columns


# Concatenating DataFrame
* Join by INDEX! (not columns as pd.merge)
pd.concat([d1, d2], axis=0, ...) // join data.frames on indices
  axis=1  // concat columns
  join='outer'  // 'inner'
  ignore_index=True
    * index of `axis` will be set to [0,...,n]
    * drops column name if axis=1
  keys=['d1', 'd2'] // create Multiindex with these outer level values
  names=['outer', 'inner']  // names of levels in Multiindex

## Stacking two frames like np.concat
* Make sure than index is equal
a.index = range(len(a))
b.index = a.index
ab = pd.concat((a, b), axis=1)


# Joining 2 DataFrames on COLUMNS
* Join by COLUMNS! (not index as pd.concat)
pd.merge(a, b, ...) // join on common columns
  on=['c1', 'c2'] // set columns explicitly; overlap otherwise
  how='left'  // right, outer, inner
    left // all rows in a; None if values not in b -> use only keys from a
    inner // removes rows in b that are also not in b -> use keys from a & b
    outer // use union a | b of keys

  suffixes=['_x', '_y']  // suffixes appended for overlapping column names
  left_index=True, right_index=True // join on indices instead of columns -> like a.join(b)


# Joining 2 DataFrames on indices
* Similar to pd.concat
a.join(b, ...)  // join on indices
  how='left'  // right, outer, inner
  rsuffix, lsuffix  // if overlapping columns names

# Appending rows
df.append([d1, d2], ...)
 * add rows
 * same as pd.concat([df, d1, d2], axis=0)
 * columns can differ -> filled with NA
 ignore_index=True  // create new index 0:nrows
df.append({'c1': 1, 'c2':2}, ignore_index=True) // append single
df.append(Series([1, 2], index=df.columns), ignore_index=True) // append single row

# Dummy coding
pd.get_dummies(series, prefix='', prefix_sep='_')
pd.get_dummies(df['c1'], prefix='c1').join(df.loc[:, df.columns != 'c1']) // dummy code columns c1




# Convert to numpy array / structured array
t = df.values // unstructured
t = df.to_records() //structured
  * encodes str as 'O', i.e. numpy object
  * not supported by hdf5

# Iterating
for index, row in df.iterrows():
  print row[colindex] // type(row) == Series
for col in df.iteritems():
  print col[rowindex]
df.iterrows() // iterate over rows
df.iteritems()  // iterator over columns
-> return tuple iterator

# R datasets
import numpy.rpy.common as com
com.load_data('mtcars')


# Grouping
g = df.groupby(['c1', 'c2'], ...)
  level=0 // group by level instead of column name
  sort=True // sort groups
  as_index=False
    * adds default index (0, ..., N-1) instead of group index as index
    * group_keys does not add an index!
  group_keys=False
    * only when calling apply
    * use to ignore group index
    * reuses original index
for name, group in groups:  // iterate over groups
g.groups  // dictionary
  .keys() // group keys
  .values() // indices! -> use get_group(key) to get the actual group
g.groups.keys() // group names
g.get_group('g1') // get group by name
g.ngroups // number of groups
g['c1', 'c2'] // select columns; return new grouped object

## Aggregation functions: reduce the dimension
g.fun
g['col'].mean(), g.col.mean() // aggregate single column
size() // # records in each group
count() // # non np.nan values in each column
mean()
median()
describe()  // summary statistics
first()
last()
nth(i)  // select record as position i
nth(0) == first()
nth(-1) == last()
nth(0, dropna='any')  // take first record without any nan
agg == aggregate
agg(np.mean)  // apply function to each column; must return unique value!
agg([np.mean, np.std]) // apply multiple functions to columns
  * Returns multi-index columns
  * use frame.columns = frame.columns.droplevel(0) to drop the top level
agg({'c1': fun1, 'c2': fun2}) // apply different functions to columns
  * NOTE: returned column order is arbitrary!

## Transform: perform group specific modifications
* return df must have same shape
d.groupby('key').transform(lambda x: x - x.mean())  // center each group
d['col'] = d.groupby('key')['col'].transform(...) // transform single column

### Difference transform / apply
* transform is applied to columns individually; apply on DataFrame
* transform must return an Iterable of the same length; apply can return anything

## Filter: select particular group (not single group members)
d.groupby('key').filter(lambda x: x.a.mean() > 0) // select groups with a.mean() > 0

## null / nan values
* Are treated as unique values
* Use df.fillna('x') to assign to unique value


## Apply: return DataFrame of any dimension
NOTE: call to first group applied twice! -> Don't use apply if long runtime
fun(d)
  * d has same columns than format than ungrouped data, only filtered by group
  * can return any DataFrame with different columns
apply(lambda df: pd.DataFrame({'f1':df.f1.min()}) // return min of f1
apply(fun, *args, **kwargs)

## Avoid passing grouping columns to apply
by = ['col1', 'col2']
g[data.columns[~d.columns.isin(by)]].apply(fun)

## Resetting index
by = ['col1', 'col2']
data.groupby(by).apply(fun).reset_index(levels=range(len(by)))
-> If apply() returns single row, return pd.Series instead of pd.DataFrame <-
  * return pd.Series(stats)
  * stats.reset_index()


## Apply functions returns a dict of metrics
def _apply_fun(df):
  return pd.Series(collections.OrderedDict(metrics))

df.groupby('key').apply(_apply_fun).reset_index()





# Comparing DataFrames / testing
df.all(axis=0)
df.all().all()
df.equals(df2)
pd.testing.assert_frame_equal(df1, df2)

# Sql
import sqlite3 as sql // mysql, etc. supported as well
con = sql.connect(file)
  file='db.sqlite'  // local file
  file=':memory'
df = pd.read_sql('SELECT * from table1', con)
df.to_sql('table1', con, ...)
  index=False // don't write index
  index_label='id'  // name of index column
  if_exists='fail|replace|append'


# Creating DataFrame
DataFrame(columns=['c1', 'c2'], index=['r1', 'r2'], dtype=bool)
  columns=  // column labels
  index=  // row labels
DataFrame({'id':[0, 1, 2], 'value':[1, 2, 3]})  // from dict
  * column order random
  * use d[[c1, c2]] to order afterwards -> no other solution (?)
DataFrame.from_dict(dict) // same as before
DataFrame(np.ones((n, m)), columns=)  // from numpy array
DataFrame([[1, 2]], columns=['a', 'b']) // from numpy array
DataFrame.from_csv

## Misc
df.values // numpy array used internally
df.values.nbytes  // memory usage
df.sort_values('column', ascending=True, inplace=False)  // sort by column(s)
  ascinding=[True, False] // multiple columns
df.sort_index(axis=0, inplace=False)  // sort by row index
df.sort_index(axis=1) // sort columns by names
df.sort_index(by='column')  // sort by column -> like sort
df.drop(['r1', 'r2'], axis=0, inplace=False) // delete rows
df.drop(['c1', 'c2'], axis=1, inplace=False) // delete columns

## hdf5
df.to_hdf('file', 'groupname')
df = pd.read_hdf('file', 'groupname')



# Series
s = Series([1, 2, 3], index=['a', 'b', 'c'], name='name')
s.index == s.keys() // index values like columns in DataFrame
s['a'], s.a
s.index = [0.1, 0.2, 0.3] // index does not need to be str
s[0.1]  // 1
for value in s: // iterate over values, not indices
  print(value)
for key, value in s.items():
  print(key, value)
s.to_frame(col_name) // Series to DataFrame COLUMNS
  s.to_frame(col_name).T // Series to DataFrame ROW
s.tolist()  // to list()
s.values  // to np.array



# Categorical
Categorical(list('bbbcc'), categories=['a', 'b', 'c', 'f'], ordered=False)
Series(list('abcd'), dtype='category')
.astype('category', categories=, ordered=False)
.categories = ['a', 'b', 'c'] // rename categories
.codes // integers
.ordered = True // set to ordered
.add_categories
.remove_categories
.rename_categories({'new': 'old})
.describe()
.to_hdf(fomart='t')  // table format required!


# Selection
.loc[rowlabel, collabel]  // purely label based
.loc[:, 'c1':'c10'] // slicing
.loc[(l1, lr2), (c1, c2)] // multiindex
.iloc[rowindex, colindex] // purely index based
.ix[rowlabel, colindex] // mixed: first label, then index
.at[rowlabel, collabel] // label based access single value
.iat[rowindex, colindex]  // fast index based access single value
.xs(value, level=1, ...) // select on specific level
  axis=1  // select columns
  drop_level=False  // retrain level
.get_value(rowlabel, collabel)  // fast label based access single value
.loc[(a & b) | (c & ~d)]  // boolean indexing by rows
loc[:, a & b] // boolean indexing by columns
.loc[df.chromo.isisn(['x', 'y'])] // isin() for Series, and indicies
d.where(d < 0, 10) // same shape as d; select d < 0 and set everything else to 10
d.mask(d < 0) // inverse where; set everythink < 0 to NA


## SettingWithCopyWarning
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

d.loc[row][col] = value // wrong; 2 __getitem__ operations; slow
d.loc[row, col] = value // correct; 1 __getitem__ operation; fast


## isin(list)
Series.isin([1, 2, 3])
columns.isin(['a', 'b'])  // works on indices
index.isin([3, 4])
## query
* Logical operations: and, or, in / &, |, ~
query('col1 < col2 & col3 == "a")
local_var = 1; query('col < @local_var') // reference local variable
query('col in [1, 2, 3]') // isin
query('col not in [1, 2, 3]') // exclude
query('col == [1, 2, 3]') // same as isin
query('col1 in col2') // same as isin

## Excluding / selecting columns
df.loc[:, ~df.columns.isin(['col1', 'col2'])]
df.drop(['col1', 'col2'], axis=1, inplace=False) // delete/remove columns
df.loc[:, 'chromo':'end']  // select all between chromo and end
df[[c for c in df.columns if c not in sel]]

# Duplicates, unique
df.duplicated(['col1', 'col2'])
df.duplicated('col').sum() == 0  // check if unique
df.drop_duplicates(['col1', 'col2'], take_last=False)

# Windowing
rolling_mean(Series, window, ...)
  * step size always 1; step_roll = rolling_mean(...)[::step]
  * ignores index -> observation are equally spaced
  * nan lead to nan -> use interpolate() first
  window=3000  // window width
  center=True // center on index; default: window ends at index
rolling_count(...)  // count non_zero entries
rolling_corr(...)
rolling_apply(df, window, fun)  // apply any function
ewma(s, span=)  // exponentially weighted moving average
ewmcorr(s, span=) // exponentially weighted moving correlation

# numba
import numba
@numba.jit
def fun_with_loops_on_large_frame(df):
  ...

# Eval
* performance improvement for large DataFrames)
* numexpr
pd.eval('df1 + df2 + 2 * df')
df.eval('a + b')  // operation on columns
df.eval('c = a + b')  // new columns
c = 2
df.eval('a + @c') // @c is local variable

# Sparse array
* lower memory
sdf = df.to_sparse(fill_value=np.nan)
sdf.density // % values != fill_value
df = sdf.to_dense()


# rpy2
import pandas.rpy.common as com
com.load_data('iris')
ro = com.convert_to_r_dataframe(df)
ro = com.convert_to_r_matrix(df)
df = com.convert_robj(ro)

## Loading RData
from rpy2.robjects import r
r('load("file.RData")')
ro = r['object']
df = com.convert_robj(ro)

## Saving to RData
-> see $PY/rpy2.txt
import pandas.rpy2.common as com
com.load_data('object') // R object -> pandas object
df_r = com.convert_to_r_dataframe(df) // first convert df
r.assign('df', df_r) // then write it to R
r('saveRDS(df, file="file.rds")')

# binning values
* return for each value[i] the bin (as category) it is assigned to
c = pd.cut(values, bins, ...) // equally spaced bins
  bins=3  // # bins
  bins=[0, 0.5, 1.0]  // (0;0.5), (0.5, 1.0) -- bin edges with minimum and maximum
  precision=2 // Float precision bin labels
  labels=['a', 'b'] // define labels of bins
  retbins=True  // return also (tuple) bins -> [0, 0.5, 1.0]
    c[0] are categories
    c[1] are tuples of cuts
  include_lowest=True // first interval is left-inclusive
pd.qcut(values, q, ...)  // same #obs in bins
  q=4 -> q=[0, 0.25, 0.5, 0.75, 1.0]
  q=[0, 0.5, 1.0] -> q=2
q = np.percentile(values, [25, 50, 75])
  * 25%, 50%, 75% percentile; not assignment!
* Returns categorical series
* Use series.cat.codes to map categories to ints








# plotting
matplotlib.style.use('ggplot')
NEW in pandas0.7
  plot.x.<X>
  plot.bar
  plot.hist
df.plot(...)
kind=line, bar, barh, hist, scatter, pie, kde, hexbin (2d density)
  bar // vertical bars
  barh  // horizonal bars
figsize=(width, height)
xlim=[a,b], ylim=
plt.xlabel('xlabel'), plt.ylabel()
legend=False  // not legend
logy=True
ax=axis
subplots=True // create subplots
layout=[nrows, ncols] // layout for subplots
sharex=False  // subplot have different x-axis
set_title(plot_title)
table=True, df  // add table to bottom; df = data.describe()
colormap='Greens', 'cubehelix'

## save figure
ax = df.plot(figsize=(width, height))
ax.get_figure().savefig('plot.pdf')

## matplotlib help
plt.xlabel(label)


## lineplot
d.loc[:, ['line1', 'line2']].plot(...)



## boxplot
df.boxplot(..)
vert=False  // horizontal
color= dict(boxes='DarkGreen', whiskers ,medians, caps)
by= // different categories become bars; subplots are columns
  columns=['c1', 'c2']  // create subplots for these columns
  df.groupby(by).boxplot()  == df.boxplot(by=by)


# subtract
a - b // subtract b from rows of a by matching indices!
a.sub(b, axis=) // same as a - b
a.subtract(b, ...)  // same as sub
a.values - b.values // broadcast numpy

# piping
* better readability
f(g(df, arg_g=1), arg_f=2)
df.pipe(g, arg_g=1).pipe(f, arg_f=2)


# pd.testing
assert_frame_equal(df1, df2, ...)

# replace / map
series.map({'a': 1, 'b': 2}) // returns None for all unmapped values
series.replace({'a': 1, 'b': 2}) // only replaces specified values


# onehot / dummy encoding
pd.get_dummies(pd.Series(list('abac')))
pd.DataFrame(dict(a=list('xyy'), b=list('yzy')))
  * dummy-encodes each column (Series) and concatenates resulting dfs


# Datetime frequencies
B        business day frequency
C        custom business day frequency
D        calendar day frequency
W        weekly frequency
M        month end frequency
SM       semi-month end frequency (15th and end of month)
BM       business month end frequency
CBM      custom business month end frequency
MS       month start frequency
SMS      semi-month start frequency (1st and 15th)
BMS      business month start frequency
CBMS     custom business month start frequency
Q        quarter end frequency
BQ       business quarter end frequency
QS       quarter start frequency
BQS      business quarter start frequency
A, Y     year end frequency
BA, BY   business year end frequency
AS, YS   year start frequency
BAS, BYS business year start frequency
BH       business hour frequency
H        hourly frequency
T, min   minutely frequency
S        secondly frequency
L, ms    milliseconds
U, us    microseconds
N        nanoseconds


# pd.to_datetime
 Return type depends on input:
  - list-like: DatetimeIndex
  - Series: Series of datetime64 dtype
  - scalar: Timestamp


series = to_datetime(pd.Series(['2010-01-01', '2010-02-01', '2010-03-01']))
series.dt // returns object to access datetime properties


# Applying functions
.apply(lambda row: row + 1, axis=1) // apply over rows or columns
.applymap(lambda x: x + 1)  // apply on single elements
.pipe(lambda df: df.iloc[:2])  // apply on entire dataframe


# Replacing columns
df.set_axis(new_columns, 1)


# Query
.query('a == a')  // remove NA values in column 'a'
.dropna(subset=['a'])  // drop rows if NA in columns 'a'

# eval
eval('new_col = other_col + @local_var')
